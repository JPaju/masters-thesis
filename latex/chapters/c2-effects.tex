\chapter{Effects} \label{Effects}

Functional programming uses immutable values and mathematical functions, also known as pure functions, to build programs. Similarly to imperative procedures, pure functions take parameters as input and compute some output. Unlike imperative procedures, however, pure functions are \textbf{only} allowed to transform their inputs to outputs and cannot have any other observable effects. Given the same inputs, a pure function must always evaluate to the same outputs. Abstraction and reuse, similarly to in imperative programs, is achieved by composing functions by passing the output of the previous function to the next function's input. The entire program can be seen as a large function composition of all functions used in the program.

A major difference between imperative and functional programming is in how one can reason about procedure or function compositions. Any expression in functional programming can always be \emph{substituted} with its value without changing the meaning of the program. The same does not apply in imperative programming. There is an implicit temporal coupling between imperative statements, since a statement may depend on the state set by previous statements. Because of this, reordering procedure calls or substituting any procedure call with its return value might completely change the meaning of the program.~\cite[Chapter~1]{sicp}

A program is considered to be \emph{referentially transparent} if it is possible to substitute an arbitrary expression in the program with its corresponding value without changing the meaning of the program in any way. Referentially transparent programs are easier to understand since they enable \emph{equational reasoning}, also known as \emph{local reasoning}. When composing pure functions, one does not have to understand their implementation, because the only effect the function is allowed to have is to return a result. A developer can only focus on the \emph{function's signature} and its specification, that is, what are the inputs and what is the output. Compilers can also take advantage of referential transparency by safely reordering expressions, evaluating expressions at compile time, memoizing results or by completely skipping the evaluation of expressions that are not required.

Referential transparency is one of the biggest differentiating factors between functional and imperative programming. Abandoning referential transparency has wide-reaching implications. In practice, it makes it much more difficult to refactor and develop programs. Developers are required to be more disciplined and to have wider knowledge of the whole program in order to not unintentionally cause defects. This is particularly evident when programming in the presence of concurrency, where side-effects can lead to race conditions and hard-to-reproduce errors.~\cite[Chapter~3]{sicp}

This chapter introduces first what effects are and discusses certain common effects in more detail. Then it presents different methods of managing effects and how these methods manifest as programming language features.  These methods include unrestricted side effects and effect systems, as well as more advanced techniques such as monadic effects and algebraic effects and handlers. The history and features relevant to managing side effects of the Scala programming language are also introduced.



\section{Effect types} \label{effect-types}
Constructing programs only by composing pure expressions without any notion of impurity is quite limiting, to say the least. To be useful in practice, programs depend on effects. An expression is said to have an effect, if its sole purpose is not to evaluate to a value or if its evaluation requires interacting with the outside world. For example printing to the console, accessing the system clock or doing IO are all examples of effects. There is no unambiguous and exact definition of what an effect is, although the concept has been given, somewhat differing, characterizations by many.

\textcite{den-lang-specs} suggest that \textquote{A complete program is thought of as an agent that interacts with the outside world, e.g., a file system, and that affects global resources, e.g., the store [mutable memory]}. They continue by stating that every phrase in a program could be classified to either a value or an effect. A value is a referentially transparent expression, while an effect is an interaction with resources allocated for the program. When an effect is encountered, the control is transferred to a \textquote{central authority}. The central authority manages the use of all resources the program has access to. They continue to describe the interaction between an effect and the central authority:
\begin{displayquote}
An effect is most easily understood as an interaction between a sub-expression
and a central authority that administers the global resources of a program. (..) Given an administrator, an effect can be viewed as a message to the central authority plus enough information to resume the suspended calculation.
\end{displayquote}

\textcite{imperative-fp} as well as \textcite{do-be-do-be-do} see the distinction between expressions and effects as \emph{being} vs. \emph{doing}. This observation is quite interesting since it brings up the concept of computations as values. Certain approaches deliberately differentiate computations from values, while some deliberately unify them. It is later discussed how separation of effects from values applies to monadic effects and algebraic effects with handlers, together with the concept of a central authority presented earlier.

% https://youtu.be/G8XMRZKOhG0?t=498
Different effects could be categorized as \emph{internal} or \emph{external}. Unlike internal effects, external effects can be observed from the outside. In the context of a whole program, the only external effect is IO, while other effects are internal. In the context of a function, matters are more complicated since effects such as mutable state, raising exceptions, and concurrency can be both internal or external, depending on the specific situation.


\subsection{Mutability} 
Mutability means that the program is able to change the state of the program, usually by mutating data stored in some memory location, and that it is possible to detect a state change by observing the changed value. Several control structures and language features require mutability. The destructive assignment operation found in almost every mainstream language is by definition mutation.~\cite[Chapter~3]{sicp} Looping constructs such as \inlinecode{for}- and \inlinecode{while} loops or iterators found in many standard libraries rely heavily on the notion of mutation. Also parts of some well known algorithms, like the swap operation in quicksort, can be expressed trivially as mutation.

In practice, almost all programs have some state that determines how the program reacts to input. Real-world examples of state include the location of characters in a game, registered users in an application and cursor position in the buffer when reading bytes from a socket. In the presence of concurrency, when parallel computations are expected to interact with each other, mutability in one form or another is needed to indicate if a computation is still on-going, completed or has encountered an error.


\subsection{Exceptions} \label{effect-types:exceptions}
Another very common effect is the ability to signal about exceptional conditions where the program is unable to compute a result or execute a command. This signaling is achieved by raising an error or exception. An exception could contain information about the condition that caused it, for example malformatted input, and that could possibly be later used when \emph{handling} or recovering from the exception. There are several common reasons why exceptions arise. They usually fall into two categories: technical or logical.~\cite{imprecise-exceptions}

Logical exceptions are usually caused by failing to meet some preconditions regarding the program's state or a function's parameters. A function may have assumptions about its inputs --- a string may need to be in a format that matches a schema in order to parse it successfully, or an integer may need to be positive and less than a certain threshold to represent a year. Sometimes inputs must be compatible with other inputs. An example of this is accessing an array by its index where the accessed index must be less than or equal to the size of the array, or attempting to access authorized content before proper authentication and authorization process.

Technical exceptions are usually related to IO, external events, the runtime environment, or the programming language itself. They can further be divided into synchronous and asynchronous exceptions. Peyton Jones describes that synchronous exceptions "arise as a direct result of some piece of code".~\cite{akward-squad}  On the other hand, asynchronous exceptions are caused by external events and they cannot always be tied to the execution of a particular line of code. In some way, logical and synchronous exceptions are expected exceptions, and asynchronous exceptions are unexpected.

Many synchronous exceptions are related to IO. If attempting to interact with a file that does not exist or the current permissions are not sufficient, the result will likely be an exception of some sort. A significant source of exceptions is communicating over the network with a remote party. Everything from name resolution, routing, transport protocol or communication schema could go wrong. A remote component in a distributed system could be completely unavailable due to a network error or an internal error in that specific component. Other types of IO problems are trying to perform an action before initialization, for example via a database connection, file descriptor or IO port.

Other synchronous exceptions may be caused by division by zero or a non-exhaustive pattern match. Probably the most well known synchronous exception is the infamous null reference error, where the program is trying to dereference a pointer that does not point to a valid memory location. In languages that support direct memory access, an attempt to access memory outside of the allowed memory range leads in a program or operating system level exception.~\cite{akward-squad}

Asynchronous exceptions usually originate from the runtime environment of the program, operating system, concurrency, or user interruption. Asynchronously raised exceptions are characterized by the fact that they could occur at an arbitrary point in time~\cite{async-exc}. An example of this is a situation where a thread interrupts the execution of another thread. The whole program could also be interrupted by a user (for example by pressing Ctrl+C) or the runtime, possibly due to a critical error in the program or operating system. Resource exhaustion is another common cause of asynchronous exceptions. Errors like stack overflow or out of memory can happen every time new memory is required from the stack or heap, thus those are categorized as asynchronous. Many environments also support dependencies to libraries that are loaded/linked  dynamically at run time. The programmer cannot always specify the exact time when dynamic loading should take place, and for this reason failing to load required dependencies could be considered an asynchronous exception.~\cite{akward-squad}

Exceptions can also encode another related and important concept, \emph{optionality}. Encoding optionality via exceptions is achieved by raising an exception that contains only a value of the unit type\footnote{A type whose cardinality is 1 (i.e., that has only one value) and thus does not contain any information.}, signaling that no result could be computed and there is no additional information about the exception. Optionality is an approriate choice instead of exceptions when the cause of the exception is trivial. Such cases include unsuccessfully querying a row from a database with specific id, searching an element from an array or trying to find a substring from a string.

Usually, the semantics of raising and handling an error are considered to interrupt the normal control flow of the program and transferring the execution to the closest appropriate \emph{exception handler}. An exception handler decides if and how to continue the execution, or whether to let the exception bubble up the layers of exception handlers. This "short-circuiting" semantics is a natural way to think and program in the presence of errors. However, the ability to raise errors from an arbitrary location can make it difficult to understand the meaning of the program and prove its correctness. It also poses challenges to ensuring that all exceptions that may be raised are handled appropriately. Lazy evaluation complicates things even further. The evaluation order in a lazily evaluated language may not be obvious to the programmer. This makes it harder to define clear semantics for exceptions.~\cite{imprecise-exceptions}

Effective and thorough exception handling is one of the most important practices in successful software engineering. Conversely, the inability to do so is one of the most significant factors that cause bugs and failures in software systems. A 2014 study by the University of Toronto studied multiple popular open source distributed software systems, such as Redis, Hadoop and Cassandra and found that a large portion (35\%) of catastrophical failures were caused by trivial mistakes in error handling code. Such mistakes include practices like omitting error handling code completely and writing a TODO-comment instead. In addition to failures, inadequate error handling may expose security vulnerabilities in the system.~\cite{simple-testing-failures}


\subsection{IO}
Programs need the ability to interact with the external world, i.e., with a user, other programs, or devices and sensors. IO is the medium to carry out these interactions. Like interaction in general, IO is often bidirectional --- the term IO is a shorthand for input and output. Input is the ability to observe changes and to receive information from other parties, output enables the program to cause changes in the environment and to dispatch information to others.

Many IO effects are about interacting with the user. Probably the most well-known and fundamental form of user interaction is to display text and graphics by changing pixels on the screen. Another common type of user interaction is via the console, which consists of printing characters to standard output and reading user input from standard input. The use of external devices such as playing sounds from speakers, recording sound from a microphone, or receiving user input from the keyboard, mouse, and touchscreen, is essential in user interaction.

In addition to user interaction, a program can also use devices for other purposes. For example, reading the time from the system clock, requesting the current temperature from a sensor, or setting a digital output to 1 or 0.

Often programs need the ability to store data that persists even when the program is restarted. This is achieved by using a device that allows reading from and writing to a non-volatile memory, such as a hard drive or memory card. Usually an operating system abstracts this persistent data store by providing a file system. However, many embedded devices still communicate directly with persistent memory devices.

The reason for a program to exist is to eventually have an effect on the surrounding world. As IO is the only way to achieve this, it fundamentally distinguishes IO from other effects. Where other effects might be useful for structuring computations and expressing computations in certain ways, IO is \emph{the reason} for programs to exist in the first place~\cite{akward-squad}. To put it the other way around, it would be impossible to detect if a program is running or not if it would not be interacting with its environment.



\section{Concurrency}
\todo{Lisätty koko concurrency-osio}\\
Programs often have many simultaneous users, all of whom should be able to use the program independently of each other. Also, it is characteristic of IO that a large proportion of time is spent waiting for a response, rather than calculating results utilizing local computing resources, mainly CPUs. If possible, several operations should be performed in parallel in order to improve performance and efficiently utilize the underlying hardware. Because of these, the programs should be able to run multiple workflows interleaved (concurrency) or at the same time (parallelism).


\subsection{Concurrency problems}
Often workflows must interact with other concurrent workflows. A parent workflow might spawn multiple child workflows and split a task between them. In some situations, one might run several workflows in parallel and choose the result that is computed the fastest, discarding all other results. Concurrent workflows sometimes must use a shared resource, like mutable memory, file, or database connection.

At first glance, these interactions may seem trivial, but concurrency complicates the program significantly. By default the execution order of concurrent workflows is nondeterministic, because of how tasks are scheduled (usually by the operating system) to run on actual hardware. Many statements in a programming language are compiled to or interpreted as several CPU instructions, executed sequentially at different clock cycles. A canonical example of this is the increment operator (\inlinecode{++} or \inlinecode{+=}), that first reads a variable's value with one instruction and then sets it to a new value with another. If another parallel workflow is updating the same variable at the same time, it might see the value between the two instructions, even though that is rarely the desired behavior. These kind of situations are called \emph{race conditions}.

In order to prevent race conditions, explicit countermeasures are required. One way is to \emph{synchronize} access to shared resources. Usually this means that before a workflow can enter a section of the program or use a shared resource, it must acquire an exclusive \emph{lock}. This means that only a one of the workflows has access to the resource at given point of time. Another way, applicable to shared memory, is to use atomic compare-and-swap operations~\cite{concurrent-queue-algorithms}, which enable to observe and update some data, succeeding only if the data was not modified by another workflow in between.

Locks and atomic references are examples of low-level tools for managing concurrency. Each tool comes with a set of trade-offs. When a workflow must acquire multiple locks, a possibility for \emph{deadlock} arises. Deadlock is a situation where concurrent workflows are blocked by each other and neither can continue until the other releases a lock they are holding. Atomic operations can fail, and the failure must be handled for example by retrying until operation succeeds. Basic atomic operations usually cannot guarantee the atomicity of operations spanning over many atomic references.


\subsection{Concurrency primitives}
\todo{Tästä osiosta on jo kommentit}
In practice concurrency can be implemented with many different constructs. The lowest-level construct commonly accessible to programming languages is a \textit{thread}. It is an \acronym{OS}{Operating System} level abstraction for concurrent execution. Each thread has its own stack, instruction pointer and \acronym{CPU}{Central Processing Unit} register values. All threads created by a single process share the same memory space, i.e. they are able to read from and write to the same shared memory blocks.

Threads are run on the actual hardware of the computer; a modern multi-core CPU can execute several workflows in parallel. The OS \textit{schedules} different threads for execution, and after a thread has been executing for a scheduled amount of time, the operating system interrupts the execution and switches the execution to a different thread. The operation where a CPU core switches the execution from one thread to another is called \textit{context switch}. Context switch requires that CPU registers and stack of the previous thread are saved, and respectively registers and stack of the new thread is loaded to the CPU.

% Threads
Traditionally a thread has been the concurrency primitive to turn to when some form of parallelism is required. Threads, however, are not a lightweight construct and they can exist only in limited numbers, usually in the thousands. Context switching between threads involves a significant amount of work, and thus causes performance overhead. Often a context switch defeats many optimizations in contemporary CPUs like caching, pipelining and speculative execution, which in turn amplifies the performance overhead. The issue manifests itself particularly in highly concurrent scenarios where computations are IO bound, which is usually the case in web and enterprise applications.

% Thread pools
A common way to constrain the total number of threads and increase their reuse is to collect multiple threads into a \emph{pool} of threads, where tasks could be submitted for execution instead of operating with individual threads. Once a task is submitted to a thread pool, it is queued and run once there are available threads. This way many concurrent workflows could be \emph{multiplexed} into a smaller number of physical threads. When fewer threads are created and reused by multiple concurrent workflows, the intent is to decrease the number of context switches in the hope of performance gains.

% Cooperative scheduling
Thread pools do not solve the problem that when a thread is waiting an IO operation or another thread to complete, the waiting thread is blocked. When a thread blocks, the OS puts it in a waiting state, meaning that its execution is not continued until the event it is waiting on is triggered. The ideal solution would be that no physical threads are blocked, and blocking is only semantic. This is not possible when the threads are preemptively scheduled by the OS. A solution is to change the scheduling model from preemptive to \emph{cooperative}. Cooperative scheduling means that when a workflow is about to be blocked, it will register itself to be scheduled once all of its dependencies are met, and yield the control to other workflows. In this model no physical threads need to be blocked.

% Event loops
Cooperative scheduling is usually implemented by a runtime environment, programming language, or library, that runs on top of preemptively scheduled OS threads. Event loop is a common pattern for implementing cooperative scheduling, and it is used extensively in asynchronous IO or single-threaded environments like JavaScript. The idea is to have a queue that contains computations waiting to get executed, and the event loop picks up and executes tasks from the queue one at a time. Once a task is about the do a blocking operation, it registers a callback. The callback is invoked when the blocking operation completes, and it will add another task to the queue that represents the remaining of the workflow.

% Fibers
Another way to implement cooperative scheduling is \textit{fibers}. They are lightweight threads that are managed and scheduled in the application instead of OS. Each fiber contains a stack and possibly error handlers or thread-local variables, similar to a thread. Fibers require a scheduler that determines what fibers to execute on actual OS threads. The scheduler can multiplex many fibers to run on smaller number of physical threads. Fibers could exist in the hundreds of thousands or millions, and switching execution from one fiber to another is very cheap in comparison to a context switch between threads. The fiber scheduler can assign a fiber to execute on a specific CPU core, which will make it easier to reap benefits from CPU optimizations like caches.


\subsection{Structured concurrency}
\todo{Tästä osiosta on jo kommentit}
When parent workflow spawns many child workflows, it is common that if one of the children encounters an error, the result cannot be computed at all and thus the results of other sibling workflows are not needed anymore. A similar situation can happen with racing workflows: when the first workflow successfully computes a result, the results of other workflows are no longer needed. In both of the situations it would be ideal to cancel the workflows whose results are not needed to preserve compute resources and make sure that no concurrent workflows remain in execution. Traditional concurrency primitives, such as threads, do not offer this kind of control out of the box.

A solution to this is \textit{structured concurrency}~\cite{structured-concurrency, go-statement-considered-harmful}, which makes it possible to define clear semantics on if and how a child workflow could outlive its parent. The basic idea of structured concurrency is that there is a way to govern how child workflows are handled when the parent workflow completes (by succeeding or failing), or when there is an error encountered in any of the sibling workflows. For example, for a parent workflow that spawns child workflows one would like to define whether the children should be awaited, cancelled, or left orphaned when the parent completes or is cancelled before the children are finished, 

Native support for structured concurrency in programming languages is still quite rare, but it has been added to programming languages at an accelerating pace. Kotlin added structured concurrency back in 2018~\cite{kotlin-sc}, Swift 5.5 in 2021~\cite{swift-sc} and Java 19 in 2022~\cite{java-sc}. The feature will probably find its way into more programming languages in the future.

It is difficult to write correct and concurrent programs. Knowledge of concurrency primitives and tools, such as different data structures and CPU instructions, and possible concurrency hazards are required. One has to be especially careful about race conditions, which are not always obvious. Ideally, concurrency could be implemented with high-level code, using operations that take into consideration possible concurrency issues, and deal with low-level details.
